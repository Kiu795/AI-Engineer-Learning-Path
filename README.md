# AI-Engineer-Learning-Path
Job Oriented Learning &amp; Reviewing Path(duration：1~3months).

用于本人实习期间记录复习&学习内容，待补充优化。

> 目标：在每天 **1–2 小时**（好时可达 3–4 小时）的条件下，系统掌握 **机器学习基础 → 深度学习 → Transformer → PyTorch/MindSpore → 大模型微调 → vLLM + MindIE 部署**。
>  每周 6 天学习、1 天复习/休息。

------

## 第1周 — 线性代数与概率回顾（打地基）

| 天数 | 日预计时长 | 任务（输出/练习）                                            |
| ---- | ---------- | ------------------------------------------------------------ |
| 周1  | 1–2h       | **向量与矩阵运算**：向量/矩阵加法、乘法、转置、单位矩阵。练：用 NumPy 实现矩阵乘法并测试。 |
| 周2  | 1–2h       | **矩阵分解**：特征值/特征向量、SVD 概念。输出：写短笔记，举例 SVD 在降维的直观应用。 |
| 周3  | 1–2h       | **概率基础**：随机变量、期望、方差、条件概率。练：写 5 道小题并解答。 |
| 周4  | 1–2h       | **概率分布**：正态、伯努利、泊松、指数。练：可视化正态分布与参数变化（Matplotlib）。 |
| 周5  | 1–2h       | **统计与估计**：MLE 概念、置信区间、简单假设检验概念。输出：例子说明 MLE。 |
| 周6  | 1–2h       | **线性回归**：普通最小二乘、矩阵形式推导。练：用 sklearn 做线性回归并解释系数。 |
| 周7  | 1–2h       | **复习/练习（整合）**：完成一小练习题集合（包括矩阵、SVD、概率、线性回归）。 |

------

## 第2周 — 经典机器学习算法（理解思想）

| 天数 | 日预计时长 | 任务（输出/练习）                                            |
| ---- | ---------- | ------------------------------------------------------------ |
| 周1  | 1–2h       | **逻辑回归**：原理、交叉熵损失、sigmoid。练：手写小规模梯度下降。 |
| 周2  | 1–2h       | **决策树 & 随机森林**：信息增益、过拟合、剪枝。练：用 sklearn 训练并可视化树。 |
| 周3  | 1–2h       | **SVM**：间隔、核 trick 概念、软间隔。输出：写一段总结对比 SVM 与 Logistic。 |
| 周4  | 1–2h       | **KNN / K-means**：原理、优劣、适用场景。练：实现简单的 k-means。 |
| 周5  | 1–2h       | **PCA 与降维**：实践：用 PCA 降维并可视化（2D）。            |
| 周6  | 1–2h       | **模型评估**：混淆矩阵、ROC/AUC、Precision/Recall。练：用实际数据计算指标。 |
| 周7  | 1–2h       | **复习/整合**：把本周学习的算法写成“概念速记卡片”。          |

------

## 第3周 — 深度学习基础（神经网络、训练流程）

| 天数 | 日预计时长 | 任务（输出/练习）                                            |
| ---- | ---------- | ------------------------------------------------------------ |
| 周1  | 1–2h       | **神经网络基础**：激活函数（ReLU, Sigmoid, Tanh），损失函数概念。 |
| 周2  | 1–2h       | **反向传播原理**：计算图、链式法则、手算一层的梯度。练：用小例子计算梯度。 |
| 周3  | 1–2h       | **优化器**：SGD、Momentum、Adam，学习率调度。练：对比 SGD 与 Adam 在一个 toy 数据集的效果（记录 loss 曲线）。 |
| 周4  | 1–2h       | **正则化技巧**：L1/L2、Dropout、BatchNorm。理解为什么能防过拟合。 |
| 周5  | 1–2h       | **CNN 基础**：卷积核、步幅、填充、池化。练：用 PyTorch 快速搭一个小 CNN（MNIST 或 CIFAR-10）。 |
| 周6  | 1–2h       | **RNN/LSTM 概念**：序列建模、梯度消失问题。输出：写一段对比 RNN 和 Transformer 的说明。 |
| 周7  | 1–2h       | **复习/小项目**：训练一个小 CNN 做分类（把结果写成短报告）。 |

------

## 第4周 — 深度学习进阶（训练工程实践）

| 天数 | 日预计时长 | 任务（输出/练习）                                            |
| ---- | ---------- | ------------------------------------------------------------ |
| 周1  | 1–2h       | **数据管道**：Dataset / DataLoader、数据增强、batching。练：写自定义 Dataset。 |
| 周2  | 1–2h       | **训练流程**：训练/验证/测试循环、早停、checkpoint 保存。实操：完整训练脚手架代码模板。 |
| 周3  | 1–2h       | **调参实务**：超参网格搜索、学习率选择、batch size 影响。练：跑 3 个超参组合并记录结果。 |
| 周4  | 1–2h       | **模型部署基础**：模型序列化（.pt/.onnx）、推理注意点（eval 模式、no_grad）。 |
| 周5  | 1–2h       | **可视化与日志**：TensorBoard / Matplotlib 用法，记录训练曲线。 |
| 周6  | 1–2h       | **工程化**：训练脚本参数化、配置管理（YAML/argparse）。练：把训练脚本参数化。 |
| 周7  | 1–2h       | **复习/小项目**：完成一个小端到端训练流程（含 checkpoint 与日志）。 |

------

## 第5周 — Transformer 原理（重点）

| 天数 | 日预计时长 | 任务（输出/练习）                                            |
| ---- | ---------- | ------------------------------------------------------------ |
| 周1  | 1–2h       | **Self-Attention 直观理解**：Q/K/V、注意力权重计算、软max。画图解释。 |
| 周2  | 1–2h       | **Multi-Head Attention**：为什么要多头、线性映射思想。练：手算小矩阵例子。 |
| 周3  | 1–2h       | **位置编码 & 层结构**：位置编码、残差连接、LayerNorm。       |
| 周4  | 1–2h       | **Encoder / Decoder 区别**：BERT vs GPT 架构对比。           |
| 周5  | 1–2h       | **Transformer 的复杂性**：时间/内存瓶颈（自注意力的 O(n²) 问题）。 |
| 周6  | 1–2h       | **优化变种简介**：FlashAttention、SparseAttention、PagedAttention（概念级）。 |
| 周7  | 1–2h       | **复习/练习**：将 Transformer 的 forward 流程写成伪代码。    |

------

## 第6周 — 手写简化 Transformer（PyTorch）

| 天数 | 日预计时长 | 任务（输出/练习）                                            |
| ---- | ---------- | ------------------------------------------------------------ |
| 周1  | 1–2h       | **搭建框架**：用 PyTorch 搭模块 skeleton（MultiHeadAttention、FeedForward）。 |
| 周2  | 1–2h       | **实现 Attention**：实现 scaled dot-product attention，写单元测试。 |
| 周3  | 1–2h       | **实现 Multi-Head + Add&Norm**：把模块组合起来。             |
| 周4  | 1–2h       | **搭建小 Transformer**：1 层 encoder 或 decoder，能做一次 forward。 |
| 周5  | 1–2h       | **训练一个小任务**：用 tiny dataset 训练短句自回归或翻译 toy task（可跑通就行）。 |
| 周6  | 1–2h       | **调试与分析**：观察 attention maps，理解模型行为。          |
| 周7  | 1–2h       | **复习/整合**：写一页总结，包含代码链接与运行说明。          |

------

## 第7周 — PyTorch 深入（实用技能）与 MindSpore 快速入门

| 天数 | 日预计时长 | 任务（输出/练习）                                            |
| ---- | ---------- | ------------------------------------------------------------ |
| 周1  | 1–2h       | **PyTorch 进阶 API**：nn.Module 设计模式、注册参数、hooks。  |
| 周2  | 1–2h       | **Mixed Precision & AMP（可选）**：理解 FP16、BF16 的利弊（笔记）。 |
| 周3  | 1–2h       | **分布式训练基础（概念）**：DataParallel vs DDP（理解足够）。 |
| 周4  | 1–2h       | **MindSpore 快速上手**：了解 Cell、construct、Dense 与常用 API（写一个小网络）。 |
| 周5  | 1–2h       | **torch_npu 概念与使用（实操）**：如何把模型 `.to("npu")`（如果你有 NPU 环境可试）。 |
| 周6  | 1–2h       | **实践**：用 PyTorch 训练一个小 Transformer（调用 DataLoader、训练循环、保存 checkpoint）。 |
| 周7  | 1–2h       | **复习/小项目**：把 PyTorch 训练脚本整理成模块化工程。       |

------

## 第8周 — 大模型微调技术（LoRA、QLoRA、Tokenization）

| 天数 | 日预计时长 | 任务（输出/练习）        |
| ---- | ---------- | ------------------------ |
| 周1  | 1–2h       | **Tokenization**：子词分 |

------

## 第9周 — RAG 基础（检索增强生成核心原理）

| 天数 | 日预计时长 | 任务（输出/练习）                                            |
| ---- | ---------- | ------------------------------------------------------------ |
| 周1  | 1–2h       | **RAG 原理**：检索增强生成（Retrieval-Augmented Generation）的整体流程理解。 |
| 周2  | 1–2h       | **文本切分（Chunking）**：固定窗口、滑动窗口、多粒度切分策略。练：对一份文档切分并分析优劣。 |
| 周3  | 1–2h       | **Embedding 向量表示**：理解 embedding 的作用、维度、相似度（cosine）。练：生成句向量并对比相似度。 |
| 周4  | 1–2h       | **向量数据库**：FAISS / Milvus / Ascend 向量检索。输出：构建本地小 FAISS 索引。 |
| 周5  | 1–2h       | **检索策略**：Sparse（BM25）vs Dense（向量）vs Hybrid。写对比总结。 |
| 周6  | 1–2h       | **RAG Pipeline 串联**：切分→Embedding→索引→检索→重写→回答。画完整流程图。 |
| 周7  | 1–2h       | **复习/小练习**：实现一个最小 RAG Demo（Python）。           |

------

## 第10周 — 昇腾 RAGSDK（企业级 RAG 实战）

| 天数 | 日预计时长 | 任务（输出/练习）                                            |
| ---- | ---------- | ------------------------------------------------------------ |
| 周1  | 1–2h       | **认识华为昇腾 RAGSDK**：组件结构、功能（embedding、indexing、retrieval、rerank）。 |
| 周2  | 1–2h       | **RAGSDK 文档解析**：熟悉官方 API，用 demo 跑通向量生成与检索。 |
| 周3  | 1–2h       | **构建索引（Indexing）**：将文档切分→Embedding→写入 RAGSDK 索引。 |
| 周4  | 1–2h       | **检索模块（Retriever）**：配置相似度、top-k、过滤器，验证检索效果。 |
| 周5  | 1–2h       | **与 MindIE + vLLM 组合**：调用模型接口 → 用 RAGSDK 检索增强 → 最终回答生成。 |
| 周6  | 1–2h       | **搭建企业级 RAG 服务 Demo**：完成端到端：上传文档 → 检索 → 推理 → 输出回答。 |
| 周7  | 1–2h       | **复习/封装项目**：整理成一个小型“文档问答系统”项目，准备进简历。 |

## 第11周 — 端到端部署演练准备

| 天数 | 日预计时长 | 任务（输出/练习）                                            |
| ---- | ---------- | ------------------------------------------------------------ |
| 周1  | 1–2h       | **设计演练项目**：确定模型（小型 GPT）、数据集、微调目标、部署目标。 |
| 周2  | 1–2h       | **训练与保存**：在本地用 PyTorch 完成训练/微调并保存 checkpoint。 |
| 周3  | 1–2h       | **导出 ONNX**：导出并验证 ONNX（用 onnxruntime 做一次推理对比）。 |
| 周4  | 1–2h       | **尝试用 vLLM 加载**：若环境支持，尝试加载权重到 vLLM（或模拟流程）。 |
| 周5  | 1–2h       | **如果有 NPU**：尝试 atc 转换到 OM（记录问题与解决方法）。   |
| 周6  | 1–2h       | **部署到 MindIE（模拟）**：写部署脚本、配置文件、服务启动命令。 |
| 周7  | 1–2h       | **复习/整合**：整理端到端运行日志与问题记录。                |

## 第12周 — 最后的整合与面试/简历准备

| 天数 | 日预计时长 | 任务（输出/练习）                                            |
| ---- | ---------- | ------------------------------------------------------------ |
| 周1  | 1–2h       | **项目整理**：把端到端项目写成 README（包含架构图与操作步骤）。 |
| 周2  | 1–2h       | **性能测试**：记录推理延迟、吞吐、内存使用（写成报告）。     |
| 周3  | 1–2h       | **问题总结**：列出遇到的 top-10 问题与解决办法（例如算子编译失败如何规避）。 |
| 周4  | 1–2h       | **面试题练习**：准备 10 道常见 ML/DL/Transformer 面试题并写答案。 |
| 周5  | 1–2h       | **简历/项目描述**：把项目写成简洁的简历条目（STAR 形式）。   |
| 周6  | 1–2h       | **展示准备**：准备 10 分钟口头演示稿，展示你做的端到端项目。 |
| 周7  | 1–2h       | **总结 & 下一步规划**：评估掌握度，列出后续要深入的 3 个方向（例如量化/分布式/算子调优）。 |



## 资源清单（推荐关键词 / 书籍 / 工具）

- **数学 & ML**：*Linear Algebra*（Gilbert Strang 笔记），*Pattern Recognition and Machine Learning*（Bishop，若需深入）
- **深度学习**：*Deep Learning*（Goodfellow）或 CS231n 课程笔记
- **PyTorch**：官方 tutorial、HuggingFace Transformers 文档
- **Transformer 理论**：原始论文 “Attention Is All You Need” + Jay Alammar 的可视化讲解（关键词查找）
- **微调**：LoRA、QLoRA（HuggingFace / peft 库）
- **vLLM / vllm-ascend**：vLLM 官方文档、社区适配指南（阅读部署示例）
- **MindIE / CANN / torch_npu**：华为官方文档（MindSpore / CANN / MindIE 用户指南）
- **实操工具**：Python, PyTorch, HuggingFace Transformers, ONNX, onnxruntime, Git, Docker
