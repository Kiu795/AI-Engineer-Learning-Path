# RAG技术原理

***

## 1.什么是RAG

*Retrieval-Augmented Generation*

RAG通过为大模型接入外部知识库，有效提升了问答系统的准确率，解决了大模型的幻觉和时效性的问题。

![image-20251119153559447](https://cdn.jsdelivr.net/gh/kiu795/pic@main/img/image-20251119153559447.png)

#### 整体流程概览：

1. **用户输入(User Input)**：用户提出一个具体问题。
2. **索引阶段 (Indexing - 离线准备)**：在系统运行前，已将大量文档处理好，生成了“chunks”和对应的“embeddings”，并存储起来。
3. **检索阶段 (Retrieval - 在线查询)**：根据用户问题，从已索引的知识库中找到最相关的几个文档片段（Chunks）。
4. **生成阶段 (Generation - LLM 推理)**：将用户问题和检索到的相关片段一起交给 LLM，让 LLM 基于这些事实信息生成最终答案。
5. **输出 (Output)**：将 LLM 生成的答案返回给用户。

#### 关键模块：

**1.Indexing**

- 过程
  - `Documents` (文档) -> 被切分成更小的 `Chunks` (块)。
  - 每个 `Chunk` 通过嵌入模型转换为 `Embeddings` (向量)。
  - 这些 `Chunks` 和 `Embeddings` 被存入数据库，以便后续快速检索。
- **目的**：为在线问答提供结构化、可搜索的知识源。

**2. Retrieval**

- **作用**：这是连接用户问题和知识库的桥梁。
- 过程
  - 用户的问题 (`Query`) 也会被转换成 `Embeddings`。
  - 系统在向量数据库中进行相似度搜索，找出与问题向量最接近的几个 `Chunks`。
  - 最终输出的是 `Relevant Documents`，即与问题最相关的几个文本片段。
- **目的**：精准定位到知识库中能回答当前问题的具体内容。

**3. Combine Context and Prompts**

- **作用**：这是 RAG 的“灵魂”所在，它决定了 LLM 如何利用检索到的信息。
- 过程
  - 将用户的原始问题 (`Question`) 和检索到的 `Chunk 1, Chunk 2, Chunk 3...` 一起打包，形成一个精心设计的提示词（Prompt）。
  - 这个 Prompt 通常会明确告诉 LLM：“请基于以下信息来回答问题”。
- **目的**：为 LLM 提供充足的、准确的上下文，引导它生成基于事实的答案，而不是依赖其内部参数化的知识或凭空捏造。

**4. LLM**

- **作用**：负责理解和推理，并最终生成自然语言答案。
- 过程
  - 接收包含问题和相关上下文的 Prompt。
  - 利用其强大的语言理解和生成能力，整合信息，生成流畅、连贯、符合逻辑的答案。
- **目的**：将检索到的“碎片化”信息，转化为用户易于理解的“完整答案”。

**5.Generation**

- **作用**：指代 LLM 生成答案的过程，是整个流程的终点。

## 2.RAG范式

![image-20251119155921931](https://cdn.jsdelivr.net/gh/kiu795/pic@main/img/image-20251119155921931.png)

### Naive RAG:

最简单的RAG方式。索引、检索、生成。

### Advanced RAG:

增加索引前后优化

索引前优化：包括优化索引结构、查询重写、扩写等。

索引后优化：检索后精排和上下文压缩。

### Modular RAG:

模块化RAG有更多的功能和更强的适应性

。新模块:路由、预测、混合检索、知识图谱。

。新模式:通过模块替换和配置来解决不同问题。

### 向量检索(Vector Search)

#### **Retrieval: 向量检索 (Vector Search)**

- **作用**：这是整个流程的核心，目的是从海量的向量中找到与 `Query Vector` 最相似的几个。
- 过程
  - 系统将 `Query Vector` 输入到 `Vector Database`。
  - 数据库内部会执行**相似度计算**（通常是余弦相似度 Cosine Similarity 或欧氏距离 Euclidean Distance）。
  - 它会计算 `Query Vector` 与数据库中每一个 `Document Vector` 的相似度得分。
  - 根据得分高低进行排序，选出得分最高的 Top-K 个向量。
- **输出**：返回这些最相似的向量所对应的原始 `Chunks` (文档片段)。

> **为什么叫“语义搜索”？** 因为它不是简单地匹配关键词，而是理解文本的“意思”。比如，用户问“Sam Altman 被解雇”，即使文档里写的是“Sam Altman was dismissed”，只要语义相近，就能被检索到。 

## 3.主流RAG对比

### LangchainChatChat

![image-20251119164227883](https://cdn.jsdelivr.net/gh/kiu795/pic@main/img/image-20251119164227883.png)

**LangchainChatChat** 的目标是：让开发者能够快速搭建一个本地化的、支持多种大模型的智能问答系统。

它的核心价值在于：

1. **易用性 (Ease of Use)**：提供了一个完整的、从数据加载到答案生成的流水线。
2. **灵活性 (Flexibility)**：支持市面上主流的开源模型和在线 API，让你可以根据需求自由选择。
3. **可视化 (Visualization)**：提供了基于 Streamlit 的 WebUI，方便非技术人员进行操作和测试。

#### **流程图解析 **

这个流程图清晰地展示了 LangchainChatChat 的内部工作流，可以分为两个主要分支：**知识库构建** 和 **在线问答**。

- **知识库构建分支 (上半部分)**：
  - `Local Documents` (本地文档) -> `Unstructured Loader` (非结构化加载器)：读取各种格式的文件（如 PDF, TXT, DOCX）。
  - `Text` -> `Text Splitter` (文本切分器)：将长文本分割成小块（Chunks），这是 RAG 的关键一步。
  - `Text Chunks` -> `Embedding` (嵌入模型)：将每个文本块转换为向量。
  - `VectorStore` (向量数据库)：存储所有向量和对应的文本块。这是知识库的核心。
- **在线问答分支 (下半部分)**：
  - `Query` (用户查询) -> `Embedding` (嵌入模型)：将问题也转换为向量。
  - `Query Vector` -> `Vector Similarity` (向量相似度计算)：在 `VectorStore` 中搜索最相似的 Top-K 个文本块。
  - `Related Text Chunks` (相关文本块) + `Prompt Template` (提示词模板) -> `Prompt` (最终提示词)：将问题和检索到的相关文本块组合成一个精心设计的 Prompt。
  - `LLM` (大语言模型)：接收 Prompt，生成最终答案。
  - `Answer` (答案)：返回给用户。

### RAG Flow

![image-20251119164541014](https://cdn.jsdelivr.net/gh/kiu795/pic@main/img/image-20251119164541014.png)

**RAGFlow** 的目标是：提供一个企业级的、全功能的 RAG 开发和部署平台，让用户可以通过图形化界面轻松构建复杂的 RAG 应用，而无需编写大量代码。

它的核心优势在于：

1. **强大的文档处理能力 (Rich Document Processing)**：
   - 支持丰富的文件类型（PDF, Word, Excel, PPT, 图片等）。
   - 提供“智能文本切分”，不仅仅是简单的按长度切分，还能理解文档结构（如标题、段落、表格），进行语义上的合理分割，确保切分后的块（Chunk）质量高，从而提升后续检索的准确性。这就是所谓的 “Quality in, quality out”。
2. **可视化的自动化工作流 (Visualized Automated Workflow)**：
   - 这是 RAGFlow 最亮眼的功能。它提供了一个类似“画布”的界面，你可以像搭积木一样，通过“拖拽”各种功能模块（如 OCR、关键词分析、向量检索、LLM 调用、条件判断、循环等）来组装自己的 RAG 流程。
   - 这种方式极大地降低了开发门槛，同时赋予了极高的灵活性，非常适合实现复杂的、多步骤的 Agentic RAG 策略。
3. **支持多种 API 和模型 (Multi-API Support)**：
   - 可以接入不同的 LLM API、Embedding 模型和向量数据库，让你可以根据性能、成本或隐私需求自由选择。

### GraphRAG

![image-20251119164633783](https://cdn.jsdelivr.net/gh/kiu795/pic@main/img/image-20251119164633783.png)

整个流程可以清晰地分为三个主要阶段：**Index 阶段 (索引)**、**构建知识图谱 (Knowledge Graph Construction)** 和 **回答问题 (Answering Questions)**。

- **Index 阶段 (索引)**：
  - `Documents` (源文档) -> `block1, block2, ..., blockN` (文本块)。
  - 这一步与传统 RAG 相同，先将文档切分成小块，以便后续处理。
- **构建知识图谱 (Knowledge Graph Construction)**：
  - **调用 LLM 自动生成顶点和边**：这是 GraphRAG 的第一个关键创新。系统会将每个文本块输入给 LLM，让 LLM 从中提取出“实体”（Entities，如人名、地点、概念）和“关系”（Relationships，如“X 是 Y 的 CEO”、“A 发生在 B 之后”），并为每个实体添加“属性值”（Attributes）。
  - **构建知识库 (Knowledge Base)**：所有提取出的实体、关系和属性被组织成一个巨大的知识图谱。图中用不同颜色的圆圈代表实体，连线代表关系，小黄点代表属性值。
- **社区检测与摘要 (Community Detection & Summarization)**：
  - **Leiden 社区发现算法**：这是一个图论算法，用于识别图谱中高度连接的“子图”，即“社区”。一个社区可以理解为围绕一个特定主题（如“OpenAI 人事变动”）聚集的一组相关实体和关系。
  - **生成社区摘要 (community summary)**：对于每一个被识别出来的社区，系统会再次调用 LLM，让它根据该社区内的所有实体和关系，生成一段精炼的摘要。这个摘要就是对该社区所代表主题的“核心观点”或“故事概要”。
- **回答问题 (Answering Questions)**：
  - `Query` (用户查询) -> `Prompt: 相关社区summary社区内的细节关系`：当用户提问时，系统不会去检索原始文本块，而是去查找与问题最相关的“社区摘要”。
  - 系统会将这些相关的社区摘要以及它们内部的“细节关系”一起打包，作为上下文提供给 LLM。
  - `LLM` (大语言模型)：接收包含社区摘要和细节关系的 Prompt，进行最终的推理和答案生成。

> **关键点**：整个过程的核心是“图谱”和“社区”。它把非结构化的文本，转化成了结构化的、可推理的知识网络，并通过社区摘要为 LLM 提供了高层次的语义背景。 



### LangGraph

![image-20251119170237794](https://cdn.jsdelivr.net/gh/kiu795/pic@main/img/image-20251119170237794.png)

#### **核心理念与价值**

**LangGraph 的目标**：让开发者能够用代码清晰、灵活地定义和控制多个 LLM 代理（Agent）或不同状态之间的交互逻辑，从而构建出复杂的、多步骤的、具有决策能力的应用。

它的核心优势在于：

1. **结构化流程**：通过图（Graph）来表示工作流，使得整个流程的逻辑一目了然，易于理解和维护。
2. **状态管理**：它天然支持状态机的概念，可以跟踪和管理 Agent 在执行过程中的“状态”，这对于需要记忆上下文或进行多轮决策的 Agent 至关重要。
3. **灵活性与可扩展性**：你可以轻松地添加、移除或修改图中的节点（Node）和边（Edge），实现高度定制化的 Agent 行为。
4. **与 LangChain 生态无缝集成**：作为 LangChain 的一部分，它可以无缝调用 LangChain 中已有的所有组件（如 LLM、工具、记忆、检索器等）。

#### **流程图解析 (中央)**

这个简单的流程图是 LangGraph 最基础的工作流模型，但它蕴含了强大的表达能力。

- **`__start__` 节点 (开始节点)**：
  - 这是整个工作流的起点，通常代表用户输入或某个触发事件。
- **`agent` 节点 (代理节点)**：
  - 这是核心节点，代表一个 LLM 代理（Agent）。它接收输入，进行思考和决策，并决定下一步要做什么。
  - `agent`节点有两个可能的输出路径：
    - **`continue` -> `tools`**：如果 Agent 决定需要使用外部工具（如搜索、计算、数据库查询等），它会将控制权交给 `tools` 节点。
    - **`end` -> `__end__`**：如果 Agent 认为任务已经完成，可以直接结束工作流。
- **`tools` 节点 (工具节点)**：
  - 代表一个或一组外部工具。这些工具执行完后，通常会将结果返回给 `agent` 节点，让 Agent 继续进行下一步决策。
  - 图中有一条从 `tools` 指向 `agent` 的箭头，这表明工作流是循环的。Agent 可以在使用工具后，再次进行思考，形成一个“思考-行动-观察-再思考”的闭环。
- **`__end__` 节点 (结束节点)**：
  - 代表工作流的终点，通常是最终答案的输出。

> **关键点**：这个图展示了一个最简单的“单 Agent + 工具调用”的循环模式。但在实际应用中，你可以在这个基础上构建出极其复杂的图，例如： 
>
> - 多个 Agent 协同工作（如一个负责规划，一个负责执行）。
> - 包含条件分支（根据不同的判断结果走向不同的路径）。
> - 包含并行处理（同时执行多个任务）。
> - 包含循环和重试机制。

#### **文字说明解析**

这部分简洁地概括了 LangGraph 的定位和功能：

- **LangGraph 是 Langchain 新出的一个成员**：
  - 它是 LangChain 生态系统中的一个重要组成部分，旨在解决传统 LangChain 链式调用（Chains）在处理复杂逻辑时的局限性。
- **利用有向无环图的方式，去协调多个 LLM 或者状态，构建 Agent 或多 Agent 的工作流**：
  - **有向无环图 (DAG)**：这是一种数学结构，由节点和有方向的边组成，且不存在循环。虽然 LangGraph 实际上也支持包含循环的图（如上图所示），但其核心思想是利用图的结构来组织和调度任务。
  - **协调多个 LLM 或者状态**：这意味着你可以定义多个不同的 LLM 实例，或者定义不同的“状态”（如“正在搜索”、“正在分析”、“等待用户确认”等），并通过图来管理它们之间的切换和协作。
  - **构建 Agent 或多 Agent 的工作流**：这是 LangGraph 的终极目标——让你能够构建出像人类一样思考和行动的智能代理。
- **项目地址 (Project Link)**：
  - `https://langchain-ai.github.io/langgraph/`：这是 LangGraph 的官方文档网站，提供了详细的教程、API 参考和示例代码。











#### 
